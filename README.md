# Welcome to Kaggle Pet Competition Course


## Part I: Machine Learning Essentials

### Week 1 (Nov 13) basic python tutorial
- [Key concepts in machine learning](https://towardsdatascience.com/machine-learning-basics-part-1-a36d38c7916)
- [How to win Kaggle competitions](https://docs.google.com/document/d/14KDMW_o1yflcZd4E0PSlKxzI68zdHG20Qz6X5wmkgSA/edit?usp=sharing)
- [Slides](https://docs.google.com/presentation/d/1cYZACKaB7e2vRZAv8Oe1GcVy6U_xBeOoeptJsl3KZtI/edit?usp=sharing)
- [Python Tutorial #1: Basics Numpy](https://github.com/amenda860111/Kaggle-Competition-Quant-and-Machine-Learning-Course-/blob/main/notebooks/tutorial_1_basic_numpy.ipynb)
- [Python Tutorial #2: Basics Pandas](https://github.com/amenda860111/Kaggle-Competition-Quant-and-Machine-Learning-Course-/blob/main/notebooks/tutorial_2_basic_pandas.ipynb)
- [Python Tutorial #3: Basic data processing and visualization](https://github.com/amenda860111/Kaggle-Competition-Quant-and-Machine-Learning-Course-/blob/main/notebooks/tutorial_3_data_preprocessing_visualization.ipynb)


### Week 2 (Aug 7-8) EDA & Feature Engineering
- [Slides](https://docs.google.com/presentation/d/13gwvLolY0Ug_WKROeVYpHpblWhNhvmj3DskSxsu3Ta0/edit?usp=sharing)
- [Slides](https://docs.google.com/presentation/d/1R8DDZf6qIG2eKTtfGcW6kph-fpTm6m3NyQkYVNk77rg/edit?usp=sharing)
- [Python Tutorial #4: Feature engineering](https://github.com/amenda860111/Kaggle-Competition-Quant-and-Machine-Learning-Course-/blob/main/notebooks/tutorial_4_feature_engineering.ipynb)


### Week 3 (Aug 14-15)
- 3.1 Cross validation, grid search for parameter selection
	- [Python Tutorial #5: Cross validation](https://github.com/amenda860111/Kaggle-Competition-Quant-and-Machine-Learning-Course-/blob/main/notebooks/tutorial_5_cross_validation.ipynb) 
- 3.2 Key machine learning models: Linear regression, Ridge, Lasso models
	- [Python Tutorial #6: Linear models](https://github.com/amenda860111/Kaggle-Competition-Quant-and-Machine-Learning-Course-/blob/main/notebooks/tutorial_6_linear%20models.ipynb)

### Week 4 (Aug 21-22)
- 4.1 Key machine learning models: Tree based models
	- [Python Tutorial #7: Decision Trees](https://github.com/amenda860111/Kaggle-Competition-Quant-and-Machine-Learning-Course-/blob/main/notebooks/tutorial_7_decision_tree.ipynb)
	- [Gini impurity in decision tree CART algorithm](https://victorzhou.com/blog/gini-impurity/)
- 4.2 Gradient boost models: Xgboost, LightGBM
	- [Python Tutorial #8: Xgboost & LightGBM](https://github.com/amenda860111/Kaggle-Competition-Quant-and-Machine-Learning-Course-/blob/main/notebooks/tutorial_8_xgboost_LightGBM.ipynb)
	- [Gradient boosting explained](https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)

### Week 5 (Aug 28-29)
- 5.1 Competition data LightGBM notebook
	- [Kaggle Notebook #3: Feature engineering and LightGBM](https://www.kaggle.com/tommy1028/lightgbm-starter-with-feature-engineering-idea)
- 5.2 Improving performance by stacking multiple ML models together
	- [Kaggle Notebook #4: LightGBM with optimized params](https://www.kaggle.com/felipefonte99/optiver-lgb-with-optimized-params)
	- [Python Tutorial #9: Stacking multiple ML models](https://github.com/amenda860111/Kaggle-Competition-Quant-and-Machine-Learning-Course-/blob/main/notebooks/tutorial_9_stacking_models.ipynb)
